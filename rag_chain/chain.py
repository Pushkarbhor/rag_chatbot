class RAGChain:
    """
    Retrieval-Augmented Generation (RAG) Chain
    Combines a retriever with an LLM to generate context-aware answers.
    """

    def __init__(self, retriever, llm):
        """
        Args:
            retriever: An object with a .retrieve(query) method that returns documents.
            llm: A language model object with a .generate(prompt) or .predict(prompt) method.
        """
        self.retriever = retriever
        self.llm = llm

    def run(self, question: str, k: int = 3):
        """
        Retrieve relevant documents and generate an answer.

        Args:
            question: User query string.
            k: Number of top documents to retrieve.

        Returns:
            str: Answer generated by the LLM.
        """
        # Step 1: Retrieve relevant documents
        retrieved_docs = self.retriever.retrieve(question, k=k)
        context = "\n".join([str(doc) for doc in retrieved_docs])

        # Step 2: Create prompt
        prompt = f"""
        You are a helpful assistant.
        Use the following context to answer the question.

        Context:
        {context}

        Question:
        {question}

        Answer clearly and concisely.
        """

        # Step 3: Generate answer using LLM
        try:
            answer = self.llm.generate(prompt)
        except AttributeError:
            answer = self.llm.predict(prompt)

        return answer
